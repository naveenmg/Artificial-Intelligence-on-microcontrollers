// Auto generated by utensor-cli

#include "uTensor/core/tensor.hpp"
#include "uTensor/ops/NnOps.hpp"
#include "cifar10_cnn_weight.hpp"
#include "uTensor/ops/MatrixOps.hpp"
#include "uTensor/ops/ArrayOps.hpp"
#include "uTensor/core/context.hpp"
#include "cifar10_cnn.hpp"
#include "uTensor/ops/MathOps.hpp"


void get_cifar10_cnn_ctx(Context& ctx, Tensor* input_0) {

{ // add tensor for placeholders
    ctx.add(input_0, "Placeholder:0", 2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_feature_map_eightbit_Placeholder__port__0_reshape_dims_0), 
            "conv/feature_map_eightbit/Placeholder__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv/feature_map_eightbit/Placeholder__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "Placeholder:0", "conv/feature_map_eightbit/Placeholder__port__0/reshape_dims:0" },
             { "conv/feature_map_eightbit/Placeholder__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_feature_map_eightbit_Placeholder__port__0_reduction_dims_0), 
            "conv/feature_map_eightbit/Placeholder__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/feature_map_eightbit/Placeholder__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv/feature_map_eightbit/Placeholder__port__0/reshape:0", "conv/feature_map_eightbit/Placeholder__port__0/reduction_dims:0" },
             { "conv/feature_map_eightbit/Placeholder__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/feature_map_eightbit/Placeholder__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv/feature_map_eightbit/Placeholder__port__0/reshape:0", "conv/feature_map_eightbit/Placeholder__port__0/reduction_dims:0" },
             { "conv/feature_map_eightbit/Placeholder__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv/feature_map_eightbit/Placeholder__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map_eightbit/Placeholder__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map_eightbit/Placeholder__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "Placeholder:0",  "conv/feature_map_eightbit/Placeholder__port__0/min:0", "conv/feature_map_eightbit/Placeholder__port__0/max:0" },
             {  "conv/feature_map_eightbit/Placeholder__port__0/quantize:0",  "conv/feature_map_eightbit/Placeholder__port__0/quantize:1", "conv/feature_map_eightbit/Placeholder__port__0/quantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({2,2,3,16}, inline_conv_Variable_0), 
            "conv/Variable:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_feature_map_eightbit_conv_Variable__port__0_reshape_dims_0), 
            "conv/feature_map_eightbit/conv/Variable__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv/feature_map_eightbit/conv/Variable__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv/Variable:0", "conv/feature_map_eightbit/conv/Variable__port__0/reshape_dims:0" },
             { "conv/feature_map_eightbit/conv/Variable__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_feature_map_eightbit_conv_Variable__port__0_reduction_dims_0), 
            "conv/feature_map_eightbit/conv/Variable__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/feature_map_eightbit/conv/Variable__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv/feature_map_eightbit/conv/Variable__port__0/reshape:0", "conv/feature_map_eightbit/conv/Variable__port__0/reduction_dims:0" },
             { "conv/feature_map_eightbit/conv/Variable__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/feature_map_eightbit/conv/Variable__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv/feature_map_eightbit/conv/Variable__port__0/reshape:0", "conv/feature_map_eightbit/conv/Variable__port__0/reduction_dims:0" },
             { "conv/feature_map_eightbit/conv/Variable__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv/feature_map_eightbit/conv/Variable__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map_eightbit/conv/Variable__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map_eightbit/conv/Variable__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv/Variable:0",  "conv/feature_map_eightbit/conv/Variable__port__0/min:0", "conv/feature_map_eightbit/conv/Variable__port__0/max:0" },
             {  "conv/feature_map_eightbit/conv/Variable__port__0/quantize:0",  "conv/feature_map_eightbit/conv/Variable__port__0/quantize:1", "conv/feature_map_eightbit/conv/Variable__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, VALID),
             { "conv/feature_map_eightbit/Placeholder__port__0/quantize:0", "conv/feature_map_eightbit/conv/Variable__port__0/quantize:0", "conv/feature_map_eightbit/Placeholder__port__0/quantize:1", "conv/feature_map_eightbit/Placeholder__port__0/quantize:2", "conv/feature_map_eightbit/conv/Variable__port__0/quantize:1", "conv/feature_map_eightbit/conv/Variable__port__0/quantize:2" },
             { "conv/feature_map/eightbit:0", "conv/feature_map/eightbit:1", "conv/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv/feature_map/eightbit:0", "conv/feature_map/eightbit:1", "conv/feature_map/eightbit:2" },
             { "conv/feature_map/eightbit/requant_range:0", "conv/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv/feature_map/eightbit:0", "conv/feature_map/eightbit:1", "conv/feature_map/eightbit:2", "conv/feature_map/eightbit/requant_range:0", "conv/feature_map/eightbit/requant_range:1" },
             { "conv/feature_map/eightbit/requantize:0", "conv/feature_map/eightbit/requantize:1", "conv/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({16}, inline_conv_bias_0), 
            "conv/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_logits_eightbit_conv_bias__port__0_reshape_dims_0), 
            "conv/logits_eightbit/conv/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv/logits_eightbit/conv/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv/bias:0", "conv/logits_eightbit/conv/bias__port__0/reshape_dims:0" },
             { "conv/logits_eightbit/conv/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_logits_eightbit_conv_bias__port__0_reduction_dims_0), 
            "conv/logits_eightbit/conv/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/logits_eightbit/conv/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv/logits_eightbit/conv/bias__port__0/reshape:0", "conv/logits_eightbit/conv/bias__port__0/reduction_dims:0" },
             { "conv/logits_eightbit/conv/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv/logits_eightbit/conv/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv/logits_eightbit/conv/bias__port__0/reshape:0", "conv/logits_eightbit/conv/bias__port__0/reduction_dims:0" },
             { "conv/logits_eightbit/conv/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv/logits_eightbit/conv/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/logits_eightbit/conv/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv/logits_eightbit/conv/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv/bias:0",  "conv/logits_eightbit/conv/bias__port__0/min:0", "conv/logits_eightbit/conv/bias__port__0/max:0" },
             {  "conv/logits_eightbit/conv/bias__port__0/quantize:0",  "conv/logits_eightbit/conv/bias__port__0/quantize:1", "conv/logits_eightbit/conv/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv/feature_map/eightbit/requantize:0", "conv/feature_map/eightbit/requantize:1", "conv/feature_map/eightbit/requantize:2", "conv/logits_eightbit/conv/bias__port__0/quantize:0", "conv/logits_eightbit/conv/bias__port__0/quantize:1",  "conv/logits_eightbit/conv/bias__port__0/quantize:2" },
             { "conv/logits/eightbit:0", "conv/logits/eightbit:1",  "conv/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv/logits/eightbit:0", "conv/logits/eightbit:1", "conv/logits/eightbit:2" },
             { "conv/logits/eightbit/requant_range:0", "conv/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv/logits/eightbit:0", "conv/logits/eightbit:1", "conv/logits/eightbit:2", "conv/logits/eightbit/requant_range:0", "conv/logits/eightbit/requant_range:1" },
             { "conv/logits/eightbit/requantize:0", "conv/logits/eightbit/requantize:1", "conv/logits/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({3,3,16,32}, inline_conv_1_Variable_quantized_const_0), 
            "conv_1/Variable_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_1_Variable_quantized_min_0), 
            "conv_1/Variable_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_1_Variable_quantized_max_0), 
            "conv_1/Variable_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv_1/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, VALID),
             { "conv/logits/eightbit/requantize:0", "conv_1/Variable_quantized_const:0", "conv/logits/eightbit/requantize:1", "conv/logits/eightbit/requantize:2", "conv_1/Variable_quantized_min:0", "conv_1/Variable_quantized_max:0" },
             { "conv_1/feature_map/eightbit:0", "conv_1/feature_map/eightbit:1", "conv_1/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_1/feature_map/eightbit:0", "conv_1/feature_map/eightbit:1", "conv_1/feature_map/eightbit:2" },
             { "conv_1/feature_map/eightbit/requant_range:0", "conv_1/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_1/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_1/feature_map/eightbit:0", "conv_1/feature_map/eightbit:1", "conv_1/feature_map/eightbit:2", "conv_1/feature_map/eightbit/requant_range:0", "conv_1/feature_map/eightbit/requant_range:1" },
             { "conv_1/feature_map/eightbit/requantize:0", "conv_1/feature_map/eightbit/requantize:1", "conv_1/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({32}, inline_conv_1_bias_0), 
            "conv_1/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_1_logits_eightbit_conv_1_bias__port__0_reshape_dims_0), 
            "conv_1/logits_eightbit/conv_1/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv_1/logits_eightbit/conv_1/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv_1/bias:0", "conv_1/logits_eightbit/conv_1/bias__port__0/reshape_dims:0" },
             { "conv_1/logits_eightbit/conv_1/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_1_logits_eightbit_conv_1_bias__port__0_reduction_dims_0), 
            "conv_1/logits_eightbit/conv_1/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_1/logits_eightbit/conv_1/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv_1/logits_eightbit/conv_1/bias__port__0/reshape:0", "conv_1/logits_eightbit/conv_1/bias__port__0/reduction_dims:0" },
             { "conv_1/logits_eightbit/conv_1/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_1/logits_eightbit/conv_1/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv_1/logits_eightbit/conv_1/bias__port__0/reshape:0", "conv_1/logits_eightbit/conv_1/bias__port__0/reduction_dims:0" },
             { "conv_1/logits_eightbit/conv_1/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv_1/bias:0",  "conv_1/logits_eightbit/conv_1/bias__port__0/min:0", "conv_1/logits_eightbit/conv_1/bias__port__0/max:0" },
             {  "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:0",  "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:1", "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv_1/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv_1/feature_map/eightbit/requantize:0", "conv_1/feature_map/eightbit/requantize:1", "conv_1/feature_map/eightbit/requantize:2", "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:0", "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:1",  "conv_1/logits_eightbit/conv_1/bias__port__0/quantize:2" },
             { "conv_1/logits/eightbit:0", "conv_1/logits/eightbit:1",  "conv_1/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_1/logits/eightbit:0", "conv_1/logits/eightbit:1", "conv_1/logits/eightbit:2" },
             { "conv_1/logits/eightbit/requant_range:0", "conv_1/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_1/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_1/logits/eightbit:0", "conv_1/logits/eightbit:1", "conv_1/logits/eightbit:2", "conv_1/logits/eightbit/requant_range:0", "conv_1/logits/eightbit/requant_range:1" },
             { "conv_1/logits/eightbit/requantize:0", "conv_1/logits/eightbit/requantize:1", "conv_1/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_1/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_1/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "conv_1/logits/eightbit/requantize:0", "conv_1/logits/eightbit/requantize:1", "conv_1/logits/eightbit/requantize:2" },
             { "conv_1/activation/eightbit:0", "conv_1/activation/eightbit:1", "conv_1/activation/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "MaxPool/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "MaxPool/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "MaxPool/eightbit:2", 1);

    ctx.push(new QuantizedMaxPoolingOp<uint8_t>(2, 2, 2, 2, VALID),
             { "conv_1/activation/eightbit:0", "conv_1/activation/eightbit:1", "conv_1/activation/eightbit:2" }, 
             { "MaxPool/eightbit:0", "MaxPool/eightbit:1",  "MaxPool/eightbit:2" });

    
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({3,3,32,32}, inline_conv_2_Variable_quantized_const_0), 
            "conv_2/Variable_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_2_Variable_quantized_min_0), 
            "conv_2/Variable_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_2_Variable_quantized_max_0), 
            "conv_2/Variable_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv_2/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 2, 2, 1 }, VALID),
             { "MaxPool/eightbit:0", "conv_2/Variable_quantized_const:0", "MaxPool/eightbit:1", "MaxPool/eightbit:2", "conv_2/Variable_quantized_min:0", "conv_2/Variable_quantized_max:0" },
             { "conv_2/feature_map/eightbit:0", "conv_2/feature_map/eightbit:1", "conv_2/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_2/feature_map/eightbit:0", "conv_2/feature_map/eightbit:1", "conv_2/feature_map/eightbit:2" },
             { "conv_2/feature_map/eightbit/requant_range:0", "conv_2/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_2/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_2/feature_map/eightbit:0", "conv_2/feature_map/eightbit:1", "conv_2/feature_map/eightbit:2", "conv_2/feature_map/eightbit/requant_range:0", "conv_2/feature_map/eightbit/requant_range:1" },
             { "conv_2/feature_map/eightbit/requantize:0", "conv_2/feature_map/eightbit/requantize:1", "conv_2/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({32}, inline_conv_2_bias_0), 
            "conv_2/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_2_logits_eightbit_conv_2_bias__port__0_reshape_dims_0), 
            "conv_2/logits_eightbit/conv_2/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv_2/logits_eightbit/conv_2/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv_2/bias:0", "conv_2/logits_eightbit/conv_2/bias__port__0/reshape_dims:0" },
             { "conv_2/logits_eightbit/conv_2/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_2_logits_eightbit_conv_2_bias__port__0_reduction_dims_0), 
            "conv_2/logits_eightbit/conv_2/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_2/logits_eightbit/conv_2/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv_2/logits_eightbit/conv_2/bias__port__0/reshape:0", "conv_2/logits_eightbit/conv_2/bias__port__0/reduction_dims:0" },
             { "conv_2/logits_eightbit/conv_2/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_2/logits_eightbit/conv_2/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv_2/logits_eightbit/conv_2/bias__port__0/reshape:0", "conv_2/logits_eightbit/conv_2/bias__port__0/reduction_dims:0" },
             { "conv_2/logits_eightbit/conv_2/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv_2/bias:0",  "conv_2/logits_eightbit/conv_2/bias__port__0/min:0", "conv_2/logits_eightbit/conv_2/bias__port__0/max:0" },
             {  "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:0",  "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:1", "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv_2/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv_2/feature_map/eightbit/requantize:0", "conv_2/feature_map/eightbit/requantize:1", "conv_2/feature_map/eightbit/requantize:2", "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:0", "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:1",  "conv_2/logits_eightbit/conv_2/bias__port__0/quantize:2" },
             { "conv_2/logits/eightbit:0", "conv_2/logits/eightbit:1",  "conv_2/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_2/logits/eightbit:0", "conv_2/logits/eightbit:1", "conv_2/logits/eightbit:2" },
             { "conv_2/logits/eightbit/requant_range:0", "conv_2/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_2/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_2/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_2/logits/eightbit:0", "conv_2/logits/eightbit:1", "conv_2/logits/eightbit:2", "conv_2/logits/eightbit/requant_range:0", "conv_2/logits/eightbit/requant_range:1" },
             { "conv_2/logits/eightbit/requantize:0", "conv_2/logits/eightbit/requantize:1", "conv_2/logits/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({3,3,32,32}, inline_conv_3_Variable_quantized_const_0), 
            "conv_3/Variable_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_3_Variable_quantized_min_0), 
            "conv_3/Variable_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_3_Variable_quantized_max_0), 
            "conv_3/Variable_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv_3/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 2, 2, 1 }, VALID),
             { "conv_2/logits/eightbit/requantize:0", "conv_3/Variable_quantized_const:0", "conv_2/logits/eightbit/requantize:1", "conv_2/logits/eightbit/requantize:2", "conv_3/Variable_quantized_min:0", "conv_3/Variable_quantized_max:0" },
             { "conv_3/feature_map/eightbit:0", "conv_3/feature_map/eightbit:1", "conv_3/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_3/feature_map/eightbit:0", "conv_3/feature_map/eightbit:1", "conv_3/feature_map/eightbit:2" },
             { "conv_3/feature_map/eightbit/requant_range:0", "conv_3/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_3/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_3/feature_map/eightbit:0", "conv_3/feature_map/eightbit:1", "conv_3/feature_map/eightbit:2", "conv_3/feature_map/eightbit/requant_range:0", "conv_3/feature_map/eightbit/requant_range:1" },
             { "conv_3/feature_map/eightbit/requantize:0", "conv_3/feature_map/eightbit/requantize:1", "conv_3/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({32}, inline_conv_3_bias_0), 
            "conv_3/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_3_logits_eightbit_conv_3_bias__port__0_reshape_dims_0), 
            "conv_3/logits_eightbit/conv_3/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv_3/logits_eightbit/conv_3/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv_3/bias:0", "conv_3/logits_eightbit/conv_3/bias__port__0/reshape_dims:0" },
             { "conv_3/logits_eightbit/conv_3/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_3_logits_eightbit_conv_3_bias__port__0_reduction_dims_0), 
            "conv_3/logits_eightbit/conv_3/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_3/logits_eightbit/conv_3/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv_3/logits_eightbit/conv_3/bias__port__0/reshape:0", "conv_3/logits_eightbit/conv_3/bias__port__0/reduction_dims:0" },
             { "conv_3/logits_eightbit/conv_3/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_3/logits_eightbit/conv_3/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv_3/logits_eightbit/conv_3/bias__port__0/reshape:0", "conv_3/logits_eightbit/conv_3/bias__port__0/reduction_dims:0" },
             { "conv_3/logits_eightbit/conv_3/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv_3/bias:0",  "conv_3/logits_eightbit/conv_3/bias__port__0/min:0", "conv_3/logits_eightbit/conv_3/bias__port__0/max:0" },
             {  "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:0",  "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:1", "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv_3/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv_3/feature_map/eightbit/requantize:0", "conv_3/feature_map/eightbit/requantize:1", "conv_3/feature_map/eightbit/requantize:2", "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:0", "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:1",  "conv_3/logits_eightbit/conv_3/bias__port__0/quantize:2" },
             { "conv_3/logits/eightbit:0", "conv_3/logits/eightbit:1",  "conv_3/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_3/logits/eightbit:0", "conv_3/logits/eightbit:1", "conv_3/logits/eightbit:2" },
             { "conv_3/logits/eightbit/requant_range:0", "conv_3/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_3/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_3/logits/eightbit:0", "conv_3/logits/eightbit:1", "conv_3/logits/eightbit:2", "conv_3/logits/eightbit/requant_range:0", "conv_3/logits/eightbit/requant_range:1" },
             { "conv_3/logits/eightbit/requantize:0", "conv_3/logits/eightbit/requantize:1", "conv_3/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_3/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_3/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "conv_3/logits/eightbit/requantize:0", "conv_3/logits/eightbit/requantize:1", "conv_3/logits/eightbit/requantize:2" },
             { "conv_3/activation/eightbit:0", "conv_3/activation/eightbit:1", "conv_3/activation/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "MaxPool_1/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "MaxPool_1/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "MaxPool_1/eightbit:2", 1);

    ctx.push(new QuantizedMaxPoolingOp<uint8_t>(2, 2, 2, 2, VALID),
             { "conv_3/activation/eightbit:0", "conv_3/activation/eightbit:1", "conv_3/activation/eightbit:2" }, 
             { "MaxPool_1/eightbit:0", "MaxPool_1/eightbit:1",  "MaxPool_1/eightbit:2" });

    
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({1,1,32,64}, inline_conv_4_Variable_quantized_const_0), 
            "conv_4/Variable_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_4_Variable_quantized_min_0), 
            "conv_4/Variable_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_4_Variable_quantized_max_0), 
            "conv_4/Variable_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv_4/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, VALID),
             { "MaxPool_1/eightbit:0", "conv_4/Variable_quantized_const:0", "MaxPool_1/eightbit:1", "MaxPool_1/eightbit:2", "conv_4/Variable_quantized_min:0", "conv_4/Variable_quantized_max:0" },
             { "conv_4/feature_map/eightbit:0", "conv_4/feature_map/eightbit:1", "conv_4/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_4/feature_map/eightbit:0", "conv_4/feature_map/eightbit:1", "conv_4/feature_map/eightbit:2" },
             { "conv_4/feature_map/eightbit/requant_range:0", "conv_4/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_4/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_4/feature_map/eightbit:0", "conv_4/feature_map/eightbit:1", "conv_4/feature_map/eightbit:2", "conv_4/feature_map/eightbit/requant_range:0", "conv_4/feature_map/eightbit/requant_range:1" },
             { "conv_4/feature_map/eightbit/requantize:0", "conv_4/feature_map/eightbit/requantize:1", "conv_4/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({64}, inline_conv_4_bias_0), 
            "conv_4/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_4_logits_eightbit_conv_4_bias__port__0_reshape_dims_0), 
            "conv_4/logits_eightbit/conv_4/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv_4/logits_eightbit/conv_4/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv_4/bias:0", "conv_4/logits_eightbit/conv_4/bias__port__0/reshape_dims:0" },
             { "conv_4/logits_eightbit/conv_4/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_4_logits_eightbit_conv_4_bias__port__0_reduction_dims_0), 
            "conv_4/logits_eightbit/conv_4/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_4/logits_eightbit/conv_4/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv_4/logits_eightbit/conv_4/bias__port__0/reshape:0", "conv_4/logits_eightbit/conv_4/bias__port__0/reduction_dims:0" },
             { "conv_4/logits_eightbit/conv_4/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_4/logits_eightbit/conv_4/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv_4/logits_eightbit/conv_4/bias__port__0/reshape:0", "conv_4/logits_eightbit/conv_4/bias__port__0/reduction_dims:0" },
             { "conv_4/logits_eightbit/conv_4/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv_4/bias:0",  "conv_4/logits_eightbit/conv_4/bias__port__0/min:0", "conv_4/logits_eightbit/conv_4/bias__port__0/max:0" },
             {  "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:0",  "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:1", "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv_4/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv_4/feature_map/eightbit/requantize:0", "conv_4/feature_map/eightbit/requantize:1", "conv_4/feature_map/eightbit/requantize:2", "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:0", "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:1",  "conv_4/logits_eightbit/conv_4/bias__port__0/quantize:2" },
             { "conv_4/logits/eightbit:0", "conv_4/logits/eightbit:1",  "conv_4/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_4/logits/eightbit:0", "conv_4/logits/eightbit:1", "conv_4/logits/eightbit:2" },
             { "conv_4/logits/eightbit/requant_range:0", "conv_4/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_4/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_4/logits/eightbit:0", "conv_4/logits/eightbit:1", "conv_4/logits/eightbit:2", "conv_4/logits/eightbit/requant_range:0", "conv_4/logits/eightbit/requant_range:1" },
             { "conv_4/logits/eightbit/requantize:0", "conv_4/logits/eightbit/requantize:1", "conv_4/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_4/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_4/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "conv_4/logits/eightbit/requantize:0", "conv_4/logits/eightbit/requantize:1", "conv_4/logits/eightbit/requantize:2" },
             { "conv_4/activation/eightbit:0", "conv_4/activation/eightbit:1", "conv_4/activation/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({1,1,64,128}, inline_conv_5_Variable_quantized_const_0), 
            "conv_5/Variable_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_5_Variable_quantized_min_0), 
            "conv_5/Variable_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv_5_Variable_quantized_max_0), 
            "conv_5/Variable_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv_5/feature_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, SAME),
             { "conv_4/activation/eightbit:0", "conv_5/Variable_quantized_const:0", "conv_4/activation/eightbit:1", "conv_4/activation/eightbit:2", "conv_5/Variable_quantized_min:0", "conv_5/Variable_quantized_max:0" },
             { "conv_5/feature_map/eightbit:0", "conv_5/feature_map/eightbit:1", "conv_5/feature_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_5/feature_map/eightbit:0", "conv_5/feature_map/eightbit:1", "conv_5/feature_map/eightbit:2" },
             { "conv_5/feature_map/eightbit/requant_range:0", "conv_5/feature_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_5/feature_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/feature_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_5/feature_map/eightbit:0", "conv_5/feature_map/eightbit:1", "conv_5/feature_map/eightbit:2", "conv_5/feature_map/eightbit/requant_range:0", "conv_5/feature_map/eightbit/requant_range:1" },
             { "conv_5/feature_map/eightbit/requantize:0", "conv_5/feature_map/eightbit/requantize:1", "conv_5/feature_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({128}, inline_conv_5_bias_0), 
            "conv_5/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_5_logits_eightbit_conv_5_bias__port__0_reshape_dims_0), 
            "conv_5/logits_eightbit/conv_5/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv_5/logits_eightbit/conv_5/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv_5/bias:0", "conv_5/logits_eightbit/conv_5/bias__port__0/reshape_dims:0" },
             { "conv_5/logits_eightbit/conv_5/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv_5_logits_eightbit_conv_5_bias__port__0_reduction_dims_0), 
            "conv_5/logits_eightbit/conv_5/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_5/logits_eightbit/conv_5/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv_5/logits_eightbit/conv_5/bias__port__0/reshape:0", "conv_5/logits_eightbit/conv_5/bias__port__0/reduction_dims:0" },
             { "conv_5/logits_eightbit/conv_5/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv_5/logits_eightbit/conv_5/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv_5/logits_eightbit/conv_5/bias__port__0/reshape:0", "conv_5/logits_eightbit/conv_5/bias__port__0/reduction_dims:0" },
             { "conv_5/logits_eightbit/conv_5/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv_5/bias:0",  "conv_5/logits_eightbit/conv_5/bias__port__0/min:0", "conv_5/logits_eightbit/conv_5/bias__port__0/max:0" },
             {  "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:0",  "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:1", "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv_5/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv_5/feature_map/eightbit/requantize:0", "conv_5/feature_map/eightbit/requantize:1", "conv_5/feature_map/eightbit/requantize:2", "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:0", "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:1",  "conv_5/logits_eightbit/conv_5/bias__port__0/quantize:2" },
             { "conv_5/logits/eightbit:0", "conv_5/logits/eightbit:1",  "conv_5/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv_5/logits/eightbit:0", "conv_5/logits/eightbit:1", "conv_5/logits/eightbit:2" },
             { "conv_5/logits/eightbit/requant_range:0", "conv_5/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv_5/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv_5/logits/eightbit:0", "conv_5/logits/eightbit:1", "conv_5/logits/eightbit:2", "conv_5/logits/eightbit/requant_range:0", "conv_5/logits/eightbit/requant_range:1" },
             { "conv_5/logits/eightbit/requantize:0", "conv_5/logits/eightbit/requantize:1", "conv_5/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv_5/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv_5/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "conv_5/logits/eightbit/requantize:0", "conv_5/logits/eightbit/requantize:1", "conv_5/logits/eightbit/requantize:2" },
             { "conv_5/activation/eightbit:0", "conv_5/activation/eightbit:1", "conv_5/activation/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({2}, inline_Reshape_shape_0), 
            "Reshape/shape:0", 
            1);
}
{
    ctx.add(new RamTensor<uint8_t>(), "Reshape/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "Reshape/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "Reshape/eightbit:2", 1);
    ctx.push(new QuantizedReshapeOp(),
              { "conv_5/activation/eightbit:0", "Reshape/shape:0", "conv_5/activation/eightbit:1", "conv_5/activation/eightbit:2" },
              { "Reshape/eightbit:0", "Reshape/eightbit:1", "Reshape/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({128,128}, inline_fully_connect_weight_quantized_const_0), 
            "fully_connect/weight_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fully_connect_weight_quantized_min_0), 
            "fully_connect/weight_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fully_connect_weight_quantized_max_0), 
            "fully_connect/weight_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "fully_connect/MatMul/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit:2", 2);
    ctx.push(new QntMatMulOp<uint8_t, uint8_t, int>(), 
             { "Reshape/eightbit:0", "Reshape/eightbit:1", "Reshape/eightbit:2", "fully_connect/weight_quantized_const:0", "fully_connect/weight_quantized_min:0",  "fully_connect/weight_quantized_max:0" },
             { "fully_connect/MatMul/eightbit:0", "fully_connect/MatMul/eightbit:1",  "fully_connect/MatMul/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect/MatMul/eightbit:0", "fully_connect/MatMul/eightbit:1", "fully_connect/MatMul/eightbit:2" },
             { "fully_connect/MatMul/eightbit/requant_range:0", "fully_connect/MatMul/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect/MatMul/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/MatMul/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect/MatMul/eightbit:0", "fully_connect/MatMul/eightbit:1", "fully_connect/MatMul/eightbit:2", "fully_connect/MatMul/eightbit/requant_range:0", "fully_connect/MatMul/eightbit/requant_range:1" },
             { "fully_connect/MatMul/eightbit/requantize:0", "fully_connect/MatMul/eightbit/requantize:1", "fully_connect/MatMul/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({128}, inline_fully_connect_bias_0), 
            "fully_connect/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_logits_eightbit_fully_connect_bias__port__0_reshape_dims_0), 
            "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "fully_connect/bias:0", "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape_dims:0" },
             { "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_logits_eightbit_fully_connect_bias__port__0_reduction_dims_0), 
            "fully_connect/logits_eightbit/fully_connect/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect/logits_eightbit/fully_connect/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape:0", "fully_connect/logits_eightbit/fully_connect/bias__port__0/reduction_dims:0" },
             { "fully_connect/logits_eightbit/fully_connect/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect/logits_eightbit/fully_connect/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "fully_connect/logits_eightbit/fully_connect/bias__port__0/reshape:0", "fully_connect/logits_eightbit/fully_connect/bias__port__0/reduction_dims:0" },
             { "fully_connect/logits_eightbit/fully_connect/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "fully_connect/bias:0",  "fully_connect/logits_eightbit/fully_connect/bias__port__0/min:0", "fully_connect/logits_eightbit/fully_connect/bias__port__0/max:0" },
             {  "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:0",  "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:1", "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "fully_connect/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "fully_connect/MatMul/eightbit/requantize:0", "fully_connect/MatMul/eightbit/requantize:1", "fully_connect/MatMul/eightbit/requantize:2", "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:0", "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:1",  "fully_connect/logits_eightbit/fully_connect/bias__port__0/quantize:2" },
             { "fully_connect/logits/eightbit:0", "fully_connect/logits/eightbit:1",  "fully_connect/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect/logits/eightbit:0", "fully_connect/logits/eightbit:1", "fully_connect/logits/eightbit:2" },
             { "fully_connect/logits/eightbit/requant_range:0", "fully_connect/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect/logits/eightbit:0", "fully_connect/logits/eightbit:1", "fully_connect/logits/eightbit:2", "fully_connect/logits/eightbit/requant_range:0", "fully_connect/logits/eightbit/requant_range:1" },
             { "fully_connect/logits/eightbit/requantize:0", "fully_connect/logits/eightbit/requantize:1", "fully_connect/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "fully_connect/logits/eightbit/requantize:0", "fully_connect/logits/eightbit/requantize:1", "fully_connect/logits/eightbit/requantize:2" },
             { "fully_connect/activation/eightbit:0", "fully_connect/activation/eightbit:1", "fully_connect/activation/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({128,64}, inline_fully_connect_1_weight_quantized_const_0), 
            "fully_connect_1/weight_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fully_connect_1_weight_quantized_min_0), 
            "fully_connect_1/weight_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fully_connect_1_weight_quantized_max_0), 
            "fully_connect_1/weight_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "fully_connect_1/MatMul/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit:2", 2);
    ctx.push(new QntMatMulOp<uint8_t, uint8_t, int>(), 
             { "fully_connect/activation/eightbit:0", "fully_connect/activation/eightbit:1", "fully_connect/activation/eightbit:2", "fully_connect_1/weight_quantized_const:0", "fully_connect_1/weight_quantized_min:0",  "fully_connect_1/weight_quantized_max:0" },
             { "fully_connect_1/MatMul/eightbit:0", "fully_connect_1/MatMul/eightbit:1",  "fully_connect_1/MatMul/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect_1/MatMul/eightbit:0", "fully_connect_1/MatMul/eightbit:1", "fully_connect_1/MatMul/eightbit:2" },
             { "fully_connect_1/MatMul/eightbit/requant_range:0", "fully_connect_1/MatMul/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_1/MatMul/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/MatMul/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect_1/MatMul/eightbit:0", "fully_connect_1/MatMul/eightbit:1", "fully_connect_1/MatMul/eightbit:2", "fully_connect_1/MatMul/eightbit/requant_range:0", "fully_connect_1/MatMul/eightbit/requant_range:1" },
             { "fully_connect_1/MatMul/eightbit/requantize:0", "fully_connect_1/MatMul/eightbit/requantize:1", "fully_connect_1/MatMul/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({64}, inline_fully_connect_1_bias_0), 
            "fully_connect_1/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_1_logits_eightbit_fully_connect_1_bias__port__0_reshape_dims_0), 
            "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "fully_connect_1/bias:0", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape_dims:0" },
             { "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_1_logits_eightbit_fully_connect_1_bias__port__0_reduction_dims_0), 
            "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape:0", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reduction_dims:0" },
             { "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reshape:0", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/reduction_dims:0" },
             { "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "fully_connect_1/bias:0",  "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/min:0", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/max:0" },
             {  "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:0",  "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:1", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "fully_connect_1/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "fully_connect_1/MatMul/eightbit/requantize:0", "fully_connect_1/MatMul/eightbit/requantize:1", "fully_connect_1/MatMul/eightbit/requantize:2", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:0", "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:1",  "fully_connect_1/logits_eightbit/fully_connect_1/bias__port__0/quantize:2" },
             { "fully_connect_1/logits/eightbit:0", "fully_connect_1/logits/eightbit:1",  "fully_connect_1/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect_1/logits/eightbit:0", "fully_connect_1/logits/eightbit:1", "fully_connect_1/logits/eightbit:2" },
             { "fully_connect_1/logits/eightbit/requant_range:0", "fully_connect_1/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_1/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect_1/logits/eightbit:0", "fully_connect_1/logits/eightbit:1", "fully_connect_1/logits/eightbit:2", "fully_connect_1/logits/eightbit/requant_range:0", "fully_connect_1/logits/eightbit/requant_range:1" },
             { "fully_connect_1/logits/eightbit/requantize:0", "fully_connect_1/logits/eightbit/requantize:1", "fully_connect_1/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_1/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_1/activation/eightbit:2", 1);
    ctx.push(new QuantizedReluOp<uint8_t, float, uint8_t>(), 
             { "fully_connect_1/logits/eightbit/requantize:0", "fully_connect_1/logits/eightbit/requantize:1", "fully_connect_1/logits/eightbit/requantize:2" },
             { "fully_connect_1/activation/eightbit:0", "fully_connect_1/activation/eightbit:1", "fully_connect_1/activation/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({64,10}, inline_fully_connect_2_weight_0), 
            "fully_connect_2/weight:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_2_MatMul_eightbit_fully_connect_2_weight__port__0_reshape_dims_0), 
            "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "fully_connect_2/weight:0", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape_dims:0" },
             { "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_2_MatMul_eightbit_fully_connect_2_weight__port__0_reduction_dims_0), 
            "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape:0", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reduction_dims:0" },
             { "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reshape:0", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/reduction_dims:0" },
             { "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "fully_connect_2/weight:0",  "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/min:0", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/max:0" },
             {  "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:0",  "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:1", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "fully_connect_2/MatMul/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit:2", 2);
    ctx.push(new QntMatMulOp<uint8_t, uint8_t, int>(), 
             { "fully_connect_1/activation/eightbit:0", "fully_connect_1/activation/eightbit:1", "fully_connect_1/activation/eightbit:2", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:0", "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:1",  "fully_connect_2/MatMul_eightbit/fully_connect_2/weight__port__0/quantize:2" },
             { "fully_connect_2/MatMul/eightbit:0", "fully_connect_2/MatMul/eightbit:1",  "fully_connect_2/MatMul/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect_2/MatMul/eightbit:0", "fully_connect_2/MatMul/eightbit:1", "fully_connect_2/MatMul/eightbit:2" },
             { "fully_connect_2/MatMul/eightbit/requant_range:0", "fully_connect_2/MatMul/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_2/MatMul/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/MatMul/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect_2/MatMul/eightbit:0", "fully_connect_2/MatMul/eightbit:1", "fully_connect_2/MatMul/eightbit:2", "fully_connect_2/MatMul/eightbit/requant_range:0", "fully_connect_2/MatMul/eightbit/requant_range:1" },
             { "fully_connect_2/MatMul/eightbit/requantize:0", "fully_connect_2/MatMul/eightbit/requantize:1", "fully_connect_2/MatMul/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({10}, inline_fully_connect_2_bias_0), 
            "fully_connect_2/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_2_logits_eightbit_fully_connect_2_bias__port__0_reshape_dims_0), 
            "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "fully_connect_2/bias:0", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape_dims:0" },
             { "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fully_connect_2_logits_eightbit_fully_connect_2_bias__port__0_reduction_dims_0), 
            "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape:0", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reduction_dims:0" },
             { "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reshape:0", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/reduction_dims:0" },
             { "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "fully_connect_2/bias:0",  "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/min:0", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/max:0" },
             {  "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:0",  "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:1", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "fully_connect_2/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "fully_connect_2/MatMul/eightbit/requantize:0", "fully_connect_2/MatMul/eightbit/requantize:1", "fully_connect_2/MatMul/eightbit/requantize:2", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:0", "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:1",  "fully_connect_2/logits_eightbit/fully_connect_2/bias__port__0/quantize:2" },
             { "fully_connect_2/logits/eightbit:0", "fully_connect_2/logits/eightbit:1",  "fully_connect_2/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fully_connect_2/logits/eightbit:0", "fully_connect_2/logits/eightbit:1", "fully_connect_2/logits/eightbit:2" },
             { "fully_connect_2/logits/eightbit/requant_range:0", "fully_connect_2/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fully_connect_2/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fully_connect_2/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fully_connect_2/logits/eightbit:0", "fully_connect_2/logits/eightbit:1", "fully_connect_2/logits/eightbit:2", "fully_connect_2/logits/eightbit/requant_range:0", "fully_connect_2/logits/eightbit/requant_range:1" },
             { "fully_connect_2/logits/eightbit/requantize:0", "fully_connect_2/logits/eightbit/requantize:1", "fully_connect_2/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>(), "fully_connect_2/logits:0");
    ctx.push(new DequantizeOp(), 
             { "fully_connect_2/logits/eightbit/requantize:0", "fully_connect_2/logits/eightbit/requantize:1", "fully_connect_2/logits/eightbit/requantize:2" },
             { "fully_connect_2/logits:0" });
}
}